ML Techniques

The ML techniques I'm most familiar with are as follows:

1. KPCA (Kernel Principle Component Analysis)
2. SVD (Singular Value Decomposition)
3. Random Forests
4. Neural Nets
5. Cross-correlation

I thought I'd write a little about how each works and the pros and cons of 
using it for the given dataset

1. KPCAs are traditionally used with unsupervised data. However, we can 
probably still use them in this dataset as a means of clustering the different 
images. To accomplish this, we can take each image (which is already in vector 
form) and run a simple KPCA (and choose a specific kernel, likely the Gaussian 
kernel as it's infinitely supported and generally friendly to work with) which 
will take the vectors into a higher dimensional feature space, calculate the 
eigenvectors in the feature space and allow us to draw non-linear decision 
boundaries between the clusters in the original space. 

This is adventageous over SVD's, which perform a similar trick (without the 
kernels and instead produces eigenvectors that are ranked by how much 
varaiance they capture--enabling demensionality reduction. Dimensionality 
reduction may not be necessary here, though, as I don't think the competition 
cares terribly much about runtime. If it does, then SVDs may be something to 
explore), as it can form much tighter decision boundaries. 

With the model built, we can take a new vector (testing image) into the 
feature space through the kernel, project it onto the calculated eigenvector 
basis and then us our (linear in the feature space) decision boundaries to 
determine which cluster it is most likely in.

I would use KPCAs for super accurate detection with little regard for speed. 

2. SVDs are similar to KPCA with the exception of dimensionality reduction (
and thus speed) that was mentioned before. The procedure would be similar (
without the kernel).

3. Random forests are really awesome. They are generally not as accurate as 
SVDs or neural nets, but are much faster. 

Effectively, random forests first bootstrap the data (sample with replacement).
These subsets together represent a “surrogate population” that approximates 
the original population by dispersing the bias in the original sample. With 
these newly formed datasets, we can bag the data.

To bag the data means to create several subsets of the data then train a model 
on these subsets. In the random forest algorithm, we create trees of these 
subsets. 

The difference between bagging and random forest is that bagging takes in one 
variable: the size of the subsets of the sample, while the random forest 
approach incorporates yet another: the number of predictors to search through 
to find the best predictor to split the data so that the trees have as little 
correlation as possible. This lends itself to an optimization problem built 
into the random forest algorithm. 

To classify a new input using a random forest, the algorithm runs the input 
through each of the trees in the forest which each return a classification and 
then returns the classification which was returned by the most number of 
trees. 

Random forests are generally really fast and pretty accurate. However, it's 
more difficult to add new data to the random forest model (as you would have 
to regenerate the model). That isn't really a concern in this application (and 
is also true for the other models listed here).

4. Neural nets are the least familiar algorithm to me in this list. My 
understanding of them is that we pass in data, which activates different 
"neurons" in the system that are transformed and adaptively weighted until the 
output neuron is activiated. Then, the system returns the elected output. 

This approach seems really awesome, and in the future I'd like to work more 
with it. 

5. Cross-correlations aren't really a ML technique, but it's often enough to
get a good detection scheme. Not to mention, it's much faster than the other
techniques listed here as it's much less computationally intensive. SVDs and 
KPCAs for example need to find the eigenvectors for large datasets which is a 
large time suck. 

In a convolution, we match the images up together by shifting the two images 
past each other to determine where the most "overlap" might occur. Then we 
determine with which image the testing image has the most in common. In this 
application, we would have to separate the training images into our individual 
digits 0-9 then create a "representative" image corresponding to each of the 
ten digits (most likely an average of sorts) and then convolute each testing 
image with the ten training images to determine which matches up the closest. 

To improve upon this, it might be possible to create an "average" of a subset 
of each digit of the training images (a little like bootstrapping in RF). 
Then convolute each of these new "averages" with the training image to spread
out the variance more. 


I'm going to use a convolution just for ease of coding rapidly. Then generate 
a KPCA depending on time remaining. The benchmark uses a random forest, but R 
kept crashing when I tried to run it--perhaps the limitations of my poor 
laptop.